"""
WhisperEngine Local Embedding Optimization
Configures optimal local models for different use cases
"""

import os
import logging
from typing import Dict, Any, Optional, List
import time
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """Configuration for local models"""
    name: str
    dimensions: int
    size_mb: float
    speed_per_sec: float
    use_case: str
    memory_mb: float

class LocalEmbeddingOptimizer:
    """Optimizes local embedding models for WhisperEngine use cases"""
    
    def __init__(self):
        """Initialize with optimal model configurations"""
        
        # Define optimal models for different use cases
        self.model_configs = {
            "fast_general": ModelConfig(
                name="all-MiniLM-L6-v2",
                dimensions=384,
                size_mb=23,
                speed_per_sec=90,
                use_case="General purpose, FAISS compatible",
                memory_mb=86
            ),
            "high_quality": ModelConfig(
                name="all-mpnet-base-v2", 
                dimensions=768,
                size_mb=420,
                speed_per_sec=50,
                use_case="High quality embeddings",
                memory_mb=380
            ),
            "latest_techniques": ModelConfig(
                name="nomic-embed-text-v1.5",
                dimensions=768,
                size_mb=550,
                speed_per_sec=35,
                use_case="Latest embedding techniques",
                memory_mb=450
            ),
            "ultra_fast": ModelConfig(
                name="all-distilroberta-v1",
                dimensions=768,
                size_mb=83,
                speed_per_sec=60,
                use_case="Fast with good quality",
                memory_mb=125
            )
        }
        
        # Emotion analysis models
        self.emotion_configs = {
            "transformer_based": {
                "name": "cardiffnlp/twitter-roberta-base-sentiment-latest",
                "speed_per_sec": 5.4,
                "size_mb": 500,
                "accuracy": "high",
                "use_case": "Detailed emotion analysis"
            },
            "ultra_fast": {
                "name": "vaderSentiment",
                "speed_per_sec": 33000,
                "size_mb": 1,
                "accuracy": "good",
                "use_case": "Real-time sentiment analysis"
            }
        }
    
    def get_optimal_config(self, use_case: str = "balanced") -> Dict[str, Any]:
        """Get optimal configuration for WhisperEngine"""
        
        recommendations = {
            "fast": {
                "embedding_model": self.model_configs["fast_general"],
                "emotion_model": self.emotion_configs["ultra_fast"],
                "rationale": "Optimized for speed and FAISS compatibility"
            },
            "balanced": {
                "embedding_model": self.model_configs["fast_general"],  # 384-dim for FAISS
                "emotion_model": self.emotion_configs["transformer_based"],
                "rationale": "Good balance of speed, quality, and compatibility"
            },
            "quality": {
                "embedding_model": self.model_configs["high_quality"],
                "emotion_model": self.emotion_configs["transformer_based"], 
                "rationale": "Maximum quality for memory and emotion analysis"
            }
        }
        
        return recommendations.get(use_case, recommendations["balanced"])
    
    def estimate_performance_gain(self) -> Dict[str, Any]:
        """Estimate performance gains from local processing"""
        
        return {
            "speed_improvement": {
                "embedding_generation": "18x faster (90 vs 5 per second)",
                "emotion_analysis": "660x faster (33K vs 50 per second)", 
                "memory_search": "40x lower latency (5ms vs 200ms)",
                "batch_processing": "Linear scaling vs API rate limits"
            },
            "resource_usage": {
                "memory_footprint": "86-450MB depending on model choice",
                "cpu_usage": "Moderate during embedding generation",
                "disk_space": "23-550MB for model storage",
                "network": "Zero - fully offline capable"
            },
            "user_experience": {
                "response_time": "Near-instant memory retrieval",
                "reliability": "No API failures or rate limits",
                "privacy": "All data processing stays local",
                "offline_capability": "Full functionality without internet"
            }
        }
    
    def create_optimized_config(self, target: str = "balanced") -> str:
        """Generate optimized environment configuration"""
        
        config = self.get_optimal_config(target)
        embedding_model = config["embedding_model"]
        
        env_config = f"""
# WhisperEngine Optimized Local Embedding Configuration
# Target: {target.upper()} - {config['rationale']}

# Embedding Configuration
USE_EXTERNAL_EMBEDDINGS=false
USE_LOCAL_MODELS=true
LOAD_FALLBACK_EMBEDDING_MODELS=true

# Primary embedding model ({embedding_model.name})
LLM_LOCAL_EMBEDDING_MODEL={embedding_model.name}
FALLBACK_EMBEDDING_MODEL={embedding_model.name}

# Performance Settings
EMBEDDING_BATCH_SIZE=16
EMBEDDING_MAX_CONCURRENT=4

# FAISS Configuration (matches {embedding_model.dimensions} dimensions)
FAISS_EMBEDDING_DIMENSION={embedding_model.dimensions}
FAISS_USE_GPU=false

# Model Performance Specs:
# - Speed: {embedding_model.speed_per_sec} embeddings/second
# - Dimensions: {embedding_model.dimensions}
# - Memory: ~{embedding_model.memory_mb}MB
# - Use case: {embedding_model.use_case}
"""
        
        return env_config

# Usage example
if __name__ == "__main__":
    optimizer = LocalEmbeddingOptimizer()
    
    print("ðŸš€ WhisperEngine Local Embedding Optimization")
    print("=" * 60)
    
    # Show all available configurations
    for use_case in ["fast", "balanced", "quality"]:
        config = optimizer.get_optimal_config(use_case)
        print(f"\n{use_case.upper()} Configuration:")
        print(f"  Embedding: {config['embedding_model'].name}")
        print(f"  Dimensions: {config['embedding_model'].dimensions}")
        print(f"  Speed: {config['embedding_model'].speed_per_sec} embed/sec")
        print(f"  Memory: {config['embedding_model'].memory_mb}MB")
        print(f"  Rationale: {config['rationale']}")
    
    # Show performance gains
    print(f"\n\nEstimated Performance Gains:")
    gains = optimizer.estimate_performance_gain()
    for category, improvements in gains.items():
        print(f"\n{category.replace('_', ' ').title()}:")
        for metric, value in improvements.items():
            print(f"  â€¢ {metric.replace('_', ' ').title()}: {value}")
    
    # Generate recommended config
    print(f"\n\nRecommended .env Configuration:")
    print(optimizer.create_optimized_config("balanced"))