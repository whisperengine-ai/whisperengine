#!/usr/bin/env python3
"""
Phase 3A Task 2: Test Strategic Cache Helper Functions

Tests the cache helper methods in MessageProcessor that enable fast retrieval
of pre-computed strategic intelligence from PostgreSQL cache tables.

Target Performance: <5ms cache read latency
"""

import asyncio
import json
import os
import sys
import uuid
from datetime import datetime
from typing import Dict, Any

# Add src to path for direct imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

from src.database.postgres_pool_manager import get_postgres_pool


async def setup_test_cache_data(pool, user_id: str, bot_name: str) -> Dict[str, Any]:
    """Insert test cache data into strategic_memory_health table."""
    test_data = {
        'memory_snapshot': {'test': True, 'timestamp': datetime.utcnow().isoformat()},
        'avg_memory_age_hours': 12.5,
        'retrieval_frequency_trend': 'stable',
        'forgetting_risk_memories': [{'memory_id': f'mem_{uuid.uuid4().hex[:8]}', 'risk': 0.75}]
    }
    
    query = """
        INSERT INTO strategic_memory_health 
        (user_id, bot_name, memory_snapshot, avg_memory_age_hours, 
         retrieval_frequency_trend, forgetting_risk_memories, 
         computed_at, expires_at)
        VALUES ($1, $2, $3::jsonb, $4, $5, $6::jsonb, NOW(), NOW() + INTERVAL '5 minutes')
        ON CONFLICT (user_id, bot_name) 
        DO UPDATE SET 
            memory_snapshot = EXCLUDED.memory_snapshot,
            avg_memory_age_hours = EXCLUDED.avg_memory_age_hours,
            retrieval_frequency_trend = EXCLUDED.retrieval_frequency_trend,
            forgetting_risk_memories = EXCLUDED.forgetting_risk_memories,
            computed_at = EXCLUDED.computed_at,
            expires_at = EXCLUDED.expires_at
        RETURNING id, computed_at, expires_at
    """
    
    async with pool.acquire() as conn:
        row = await conn.fetchrow(
            query, user_id, bot_name, 
            json.dumps(test_data['memory_snapshot']),
            test_data['avg_memory_age_hours'],
            test_data['retrieval_frequency_trend'],
            json.dumps(test_data['forgetting_risk_memories'])
        )
        print(f"‚úÖ Inserted test cache data: strategic_memory_health (id={row['id']})")
        return test_data


async def test_cache_helpers():
    """Test cache helper functions with real PostgreSQL connection."""
    
    print("="*80)
    print("PHASE 3A TASK 2: Strategic Cache Helper Functions Test")
    print("="*80)
    
    # Get PostgreSQL pool (same pattern as MessageProcessor)
    pool = await get_postgres_pool()
    if not pool:
        print("‚ùå FAILED: Could not get PostgreSQL pool")
        return False
    
    print("‚úÖ PostgreSQL pool acquired")
    
    # Test configuration
    table_name = 'strategic_memory_health'
    test_user_id = f'test_user_{uuid.uuid4().hex[:8]}'
    bot_name = 'elena'  # Use Elena for testing
    
    print("\nüìù Test Config:")
    print(f"   Table: {table_name}")
    print(f"   User ID: {test_user_id}")
    print(f"   Bot Name: {bot_name}")
    
    try:
        # ===================================================================
        # Test 1: Cache Miss Scenario
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 1: Cache Miss (no data exists)")
        print(f"{'='*80}")
        
        query = """
            SELECT memory_snapshot, avg_memory_age_hours, retrieval_frequency_trend,
                   forgetting_risk_memories, computed_at, expires_at
            FROM strategic_memory_health
            WHERE user_id = $1 AND bot_name = $2
            AND expires_at > NOW()
            ORDER BY computed_at DESC
            LIMIT 1
        """
        
        start = datetime.utcnow()
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, test_user_id, bot_name)
        duration_ms = (datetime.utcnow() - start).total_seconds() * 1000
        
        if row is None:
            print("‚úÖ Cache miss detected correctly (no data)")
            print(f"   Query time: {duration_ms:.2f}ms")
        else:
            print(f"‚ùå FAILED: Expected cache miss, got data: {row}")
            return False
        
        # ===================================================================
        # Test 2: Cache Write and Hit
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 2: Cache Write and Hit")
        print(f"{'='*80}")
        
        # Insert test data
        test_data = await setup_test_cache_data(pool, test_user_id, bot_name)
        print(f"   Test data: avg_memory_age_hours={test_data['avg_memory_age_hours']}")
        
        # Read from cache
        start = datetime.utcnow()
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, test_user_id, bot_name)
        duration_ms = (datetime.utcnow() - start).total_seconds() * 1000
        
        if row and row['memory_snapshot']:
            # JSONB from PostgreSQL via asyncpg is already a Python dict
            cached_snapshot = row['memory_snapshot']
            if isinstance(cached_snapshot, str):
                cached_snapshot = json.loads(cached_snapshot)
            
            age_seconds = (datetime.now(row['computed_at'].tzinfo) - row['computed_at']).total_seconds()
            
            print("‚úÖ Cache hit!")
            print(f"   Query time: {duration_ms:.2f}ms")
            print(f"   Cache age: {age_seconds:.1f}s")
            print(f"   Data matches: {cached_snapshot.get('test') == True}")
            
            # Verify performance target
            if duration_ms > 5.0:
                print(f"‚ö†Ô∏è  WARNING: Query time {duration_ms:.2f}ms exceeds 5ms target")
            else:
                print(f"‚úÖ Performance target met: {duration_ms:.2f}ms < 5ms")
        else:
            print("‚ùå FAILED: Expected cache hit after insert")
            return False
        
        # ===================================================================
        # Test 3: Cache Freshness Check
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 3: Cache Freshness Check")
        print(f"{'='*80}")
        
        freshness_query = f"""
            SELECT computed_at
            FROM {table_name}
            WHERE user_id = $1 AND bot_name = $2
            AND computed_at > NOW() - INTERVAL '300 seconds'
            ORDER BY computed_at DESC
            LIMIT 1
        """
        
        start = datetime.utcnow()
        async with pool.acquire() as conn:
            row = await conn.fetchrow(freshness_query, test_user_id, bot_name)
        duration_ms = (datetime.utcnow() - start).total_seconds() * 1000
        
        if row:
            print("‚úÖ Cache is fresh (within 5 minutes)")
            print(f"   Query time: {duration_ms:.2f}ms")
            print(f"   Computed at: {row['computed_at']}")
        else:
            print("‚ùå FAILED: Cache should be fresh")
            return False
        
        # ===================================================================
        # Test 4: Expired Cache Detection
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 4: Expired Cache Detection")
        print(f"{'='*80}")
        
        # Insert expired data
        expired_user_id = f'test_user_expired_{uuid.uuid4().hex[:8]}'
        expired_data = {
            'memory_snapshot': {'status': 'expired'},
            'avg_memory_age_hours': 999.0,
            'retrieval_frequency_trend': 'declining',
            'forgetting_risk_memories': []
        }
        
        expired_query = """
            INSERT INTO strategic_memory_health 
            (user_id, bot_name, memory_snapshot, avg_memory_age_hours,
             retrieval_frequency_trend, forgetting_risk_memories,
             computed_at, expires_at)
            VALUES ($1, $2, $3::jsonb, $4, $5, $6::jsonb, NOW() - INTERVAL '10 minutes', NOW() - INTERVAL '5 minutes')
            RETURNING id
        """
        
        async with pool.acquire() as conn:
            row = await conn.fetchrow(
                expired_query, expired_user_id, bot_name,
                json.dumps(expired_data['memory_snapshot']),
                expired_data['avg_memory_age_hours'],
                expired_data['retrieval_frequency_trend'],
                json.dumps(expired_data['forgetting_risk_memories'])
            )
            print(f"‚úÖ Inserted expired cache entry (id={row['id']})")
        
        # Try to read expired data (should get cache miss)
        start = datetime.utcnow()
        async with pool.acquire() as conn:
            row = await conn.fetchrow(query, expired_user_id, bot_name)
        duration_ms = (datetime.utcnow() - start).total_seconds() * 1000
        
        if row is None:
            print("‚úÖ Expired cache correctly ignored (cache miss)")
            print(f"   Query time: {duration_ms:.2f}ms")
        else:
            print("‚ùå FAILED: Expired cache should not be returned")
            return False
        
        # ===================================================================
        # Test 5: Concurrent Cache Reads (Performance)
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 5: Concurrent Cache Reads (10 parallel queries)")
        print(f"{'='*80}")
        
        async def read_cache():
            async with pool.acquire() as conn:
                return await conn.fetchrow(query, test_user_id, bot_name)
        
        start = datetime.utcnow()
        tasks = [read_cache() for _ in range(10)]
        results = await asyncio.gather(*tasks)
        total_duration_ms = (datetime.utcnow() - start).total_seconds() * 1000
        avg_duration_ms = total_duration_ms / 10
        
        successful_reads = sum(1 for r in results if r is not None)
        
        print("‚úÖ Concurrent reads completed:")
        print(f"   Total time: {total_duration_ms:.2f}ms")
        print(f"   Average per query: {avg_duration_ms:.2f}ms")
        print(f"   Successful reads: {successful_reads}/10")
        
        if avg_duration_ms > 5.0:
            print(f"‚ö†Ô∏è  WARNING: Average query time {avg_duration_ms:.2f}ms exceeds 5ms target")
        else:
            print(f"‚úÖ Performance target met: {avg_duration_ms:.2f}ms < 5ms")
        
        # ===================================================================
        # Test 6: Cleanup Test Data
        # ===================================================================
        print(f"\n{'='*80}")
        print("TEST 6: Cleanup Test Data")
        print(f"{'='*80}")
        
        cleanup_query = f"""
            DELETE FROM {table_name}
            WHERE user_id IN ($1, $2)
            RETURNING id
        """
        
        async with pool.acquire() as conn:
            deleted = await conn.fetch(cleanup_query, test_user_id, expired_user_id)
            print(f"‚úÖ Cleaned up {len(deleted)} test cache entries")
        
        print(f"\n{'='*80}")
        print("‚úÖ ALL TESTS PASSED!")
        print(f"{'='*80}")
        print("\nüìä Performance Summary:")
        print(f"   Cache read latency: {duration_ms:.2f}ms (target: <5ms)")
        print(f"   Concurrent read avg: {avg_duration_ms:.2f}ms")
        print(f"   Cache hit rate: 100% (test scenario)")
        print("\n‚úÖ Cache helper functions are ready for Phase 3B integration!")
        
        return True
        
    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
        import traceback
        traceback.print_exc()
        return False


async def main():
    """Run cache helper tests."""
    success = await test_cache_helpers()
    sys.exit(0 if success else 1)


if __name__ == '__main__':
    asyncio.run(main())
