#!/usr/bin/env python3
"""Test Phi-3-Mini with actual WhisperEngine system prompt."""

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from env_manager import load_environment
from src.llm.llm_client import LLMClient

def test_phi3_with_full_prompt():
    """Test Phi-3-Mini with the actual WhisperEngine system prompt."""
    print("üß™ Testing Phi-3-Mini with full WhisperEngine system prompt...")
    
    # Load environment
    if not load_environment():
        print("‚ùå Failed to load environment")
        return False
    
    # Read the actual system prompt
    with open('system_prompt.md', 'r') as f:
        system_prompt = f.read()
    
    # Check configuration
    api_url = os.getenv('LLM_CHAT_API_URL')
    local_model = os.getenv('LOCAL_LLM_MODEL')
    
    print(f"üìä Configuration:")
    print(f"   API URL: {api_url}")
    print(f"   Local Model: {local_model}")
    print(f"   System prompt: {len(system_prompt):,} chars (~{len(system_prompt)//4:,} tokens)")
    
    try:
        print("\nüîÑ Initializing Phi-3-Mini LLM client...")
        llm_client = LLMClient()
        
        # Test with shorter system prompt first
        short_system = "You are Dream of the Endless. Respond with wisdom and empathy."
        user_message = "Hello Dream, can you tell me about the nature of dreams?"
        
        print("\nüîÑ Testing with short system prompt...")
        short_messages = [
            {"role": "system", "content": short_system},
            {"role": "user", "content": user_message}
        ]
        
        response = llm_client.generate_chat_completion(short_messages, max_tokens=100)
        
        if response and 'choices' in response and len(response['choices']) > 0:
            response_text = response['choices'][0]['message']['content']
            print(f"‚úÖ Short prompt test PASSED")
            print(f"   Response: {response_text[:150]}...")
        else:
            print(f"‚ùå Short prompt test failed: {response}")
            return False
        
        # Now test with a portion of the full system prompt
        print(f"\nüîÑ Testing with medium system prompt (~1000 tokens)...")
        medium_system = system_prompt[:4000]  # ~1000 tokens
        medium_messages = [
            {"role": "system", "content": medium_system},
            {"role": "user", "content": user_message}
        ]
        
        response = llm_client.generate_chat_completion(medium_messages, max_tokens=100)
        
        if response and 'choices' in response and len(response['choices']) > 0:
            response_text = response['choices'][0]['message']['content']
            print(f"‚úÖ Medium prompt test PASSED")
            print(f"   Response: {response_text[:150]}...")
        else:
            print(f"‚ùå Medium prompt test failed")
            return False
        
        # Test with full system prompt
        print(f"\nüîÑ Testing with FULL system prompt (~3,837 tokens)...")
        full_messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # Estimate total tokens
        total_chars = len(system_prompt) + len(user_message)
        estimated_tokens = total_chars // 4
        
        print(f"   Total estimated tokens: {estimated_tokens:,}")
        print(f"   Phi-3-Mini limit: 4,096 tokens")
        print(f"   Status: {'‚úÖ Within limit' if estimated_tokens <= 4096 else '‚ùå Over limit'}")
        
        response = llm_client.generate_chat_completion(full_messages, max_tokens=200)
        
        if response and 'choices' in response and len(response['choices']) > 0:
            response_text = response['choices'][0]['message']['content']
            print(f"‚úÖ FULL PROMPT TEST PASSED! üéâ")
            print(f"   Phi-3-Mini successfully handled {estimated_tokens:,} token prompt")
            print(f"   Response: {response_text[:200]}...")
            return True
        else:
            print(f"‚ùå Full prompt test failed: {response}")
            return False
            
    except Exception as e:
        print(f"‚ùå Phi-3-Mini test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = test_phi3_with_full_prompt()
    print(f"\n{'‚úÖ Phi-3-Mini upgrade SUCCESS' if success else '‚ùå Phi-3-Mini upgrade FAILED'}")
    sys.exit(0 if success else 1)