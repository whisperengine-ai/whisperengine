# External API Emotional Intelligence Integration Guide

## Overview
This guide shows how to integrate the 3-tier external API emotion system with your existing Discord bot, designed specifically for Docker deployment on Apple Silicon without GPU dependencies.

## üéØ Your Constraints Solved
- ‚úÖ **Docker on Apple Silicon**: No local GPU/torch dependencies
- ‚úÖ **External APIs**: All models called remotely (LM Studio, Hugging Face, OpenAI)
- ‚úÖ **2-Tier Flexibility**: Light/Heavy for resource tuning
- ‚úÖ **Phase 3 Compatible**: Integrates with your existing memory system

## üìä Tier Comparison

| Tier | Accuracy | Latency | API Calls | Best For | Cost |
|------|----------|---------|-----------|----------|------|
| **Light** | 85-90% | ~100ms | 1 | High-volume chat | Low |
| **Medium** | 92-95% | ~300ms | 2 | Most interactions | Medium |
| **Heavy** | 96-98% | ~600ms | 3 | Critical moments | High |

## üöÄ Quick Integration

### 1. Add to Your Bot Class (Security-Optimized Version)

```python
from optimized_external_api_emotion_ai import OptimizedExternalAPIEmotionAI

class YourDiscordBot:
    def __init__(self):
        # Your existing initialization
        self.lm_client = LMStudioClient(...)
        self.memory_manager = UserMemoryManager(...)
        
        # Add optimized emotion AI with security safeguards
        self.emotion_ai = OptimizedExternalAPIEmotionAI(
            lm_studio_url="http://localhost:1234",  # Your LM Studio
            huggingface_api_key=os.getenv('HUGGINGFACE_API_KEY'),
            enable_optimizations=True  # Enable performance optimizations
        )
        
        # Default tier (can be changed per message)
        self.emotion_tier = 'medium'
    
    async def initialize(self):
        # Your existing initialization
        await self.emotion_ai.initialize()
        
        # Log performance baseline
        metrics = self.emotion_ai.get_performance_metrics()
        print(f"üöÄ Emotion AI initialized with optimizations: {metrics['optimizations_enabled']}")
```

### 2. Modify Your Message Handler (Performance Optimized)

```python
async def handle_message(self, message):
    user_id = str(message.author.id)
    text = message.content
    
    # Get conversation history from your existing memory system
    conversation_history = await self.memory_manager.get_recent_messages(user_id, limit=10)
    
    # Smart tier selection based on context and performance
    tier = self.choose_emotion_tier_optimized(text, user_id)
    
    # Analyze emotion with optimized external APIs
    start_time = time.time()
    emotion_analysis = await self.emotion_ai.analyze_emotion_cloud(
        text=text,
        user_id=user_id,
        conversation_history=conversation_history,
        tier=tier
    )
    analysis_time = (time.time() - start_time) * 1000
    
    # Log performance if analysis took too long
    if analysis_time > 1000:  # > 1 second
        print(f"‚ö†Ô∏è Slow emotion analysis: {analysis_time:.1f}ms (tier: {tier})")
    
    # Get Phase 3 context if available
    phase3_context = await self.get_phase3_context(user_id)
    
    # Build emotional system prompt (optimized)
    emotional_prompt = self.emotion_ai.build_cloud_emotional_prompt(
        emotion_analysis=emotion_analysis,
        phase3_context=phase3_context
    )
    
    # Generate response with optimized temperature
    response = await self.lm_client.generate_response(
        prompt=emotional_prompt,
        message=text,
        temperature=self.get_emotional_temperature_optimized(emotion_analysis)
    )
    
    # Store in memory and send
    await self.memory_manager.store_interaction(user_id, text, response, emotion_analysis)
    await message.reply(response)

def choose_emotion_tier_optimized(self, text, user_id):
    """Optimized tier selection with performance considerations"""
    
    # Check if user is rate limited - use faster tier
    if self.is_user_rate_limited(user_id):
        return 'light'
    
    # Use heavy tier for emotional crisis (critical situations)
    crisis_keywords = ['suicide', 'depression', 'hurt myself', 'give up', 'devastated']
    if any(keyword in text.lower() for keyword in crisis_keywords):
        return 'heavy'
    
    # Use heavy tier for important milestones
    milestone_keywords = ['graduation', 'wedding', 'funeral', 'birthday', 'anniversary']
    if any(keyword in text.lower() for keyword in milestone_keywords):
        return 'heavy'
    
    # Use light tier for very short messages (< 10 words)
    if len(text.split()) < 10:
        return 'light'
    
    # Use light tier during high load periods
    current_hour = datetime.now().hour
    if 18 <= current_hour <= 22:  # Peak hours
        return 'light'
    
    # Default to medium for balanced performance
    return 'medium'

def get_emotional_temperature_optimized(self, emotion_analysis):
    """Optimized emotional temperature with circuit breaker awareness"""
    
    primary_emotion = emotion_analysis.get('primary_emotion', 'neutral')
    intensity = emotion_analysis.get('intensity', 0.5)
    confidence = emotion_analysis.get('confidence', 0.5)
    
    # Base temperatures (optimized for performance)
    base_temps = {
        'joy': 0.8,      # Be more creative with happy users
        'sadness': 0.5,  # Be more careful and consistent
        'anger': 0.4,    # Be very measured and calm
        'fear': 0.5,     # Be reassuring and consistent
        'neutral': 0.7   # Balanced
    }
    
    base_temp = base_temps.get(primary_emotion, 0.7)
    
    # Adjust for intensity and confidence
    if intensity > 0.8 and confidence > 0.7:
        return max(base_temp - 0.2, 0.3)  # Lower for intense, confident emotions
    elif confidence < 0.3:
        return 0.7  # Default temperature for uncertain emotions
    
    return base_temp

def is_user_rate_limited(self, user_id):
    """Check if user should get faster processing due to rate limiting"""
    
    # Implement your rate limiting logic here
    # Return True if user should get priority/faster processing
    return False

async def get_phase3_context(self, user_id):
    """Optimized Phase 3 context retrieval"""
    
    if not hasattr(self, 'phase3_context_cache'):
        self.phase3_context_cache = {}
    
    # Check cache first (5-minute TTL)
    cache_key = f"phase3_{user_id}"
    if cache_key in self.phase3_context_cache:
        cached_time, cached_data = self.phase3_context_cache[cache_key]
        if (datetime.now() - cached_time).total_seconds() < 300:  # 5 minutes
            return cached_data
    
    # Get fresh context
    try:
        context = await self.phase3_memory.get_user_context(user_id)
        self.phase3_context_cache[cache_key] = (datetime.now(), context)
        return context
    except Exception as e:
        print(f"‚ö†Ô∏è Phase 3 context failed: {e}")
        return None
```

## üê≥ Docker Configuration

### docker-compose.yml (Security-Performance Optimized)
```yaml
version: '3.8'

services:
  discord-bot:
    build: .
    environment:
      # Discord
      - DISCORD_TOKEN=${DISCORD_TOKEN}
      
      # Emotion AI Configuration (Optimized)
      - EMOTION_TIER=heavy                    # light/heavy
      - EMOTION_OPTIMIZATIONS=true             # Enable performance optimizations
      - EMOTION_CACHE_SIZE=1000               # Larger cache for better performance
      - EMOTION_CACHE_TTL=3600                # 1 hour cache TTL
      - LM_STUDIO_URL=http://lm-studio:1234   # Internal Docker network
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}      # Optional
      
      # Performance Tuning
      - EMOTION_TIMEOUT_LIGHT=10              # Light tier timeout (seconds)
      - EMOTION_TIMEOUT_MEDIUM=15             # Medium tier timeout
      - EMOTION_TIMEOUT_HEAVY=25              # Heavy tier timeout
      - CIRCUIT_BREAKER_THRESHOLD=5           # API failure threshold
      - CIRCUIT_BREAKER_TIMEOUT=300           # Reset timeout (seconds)
      
      # Your existing config
      - USE_EXTERNAL_EMBEDDINGS=true
      - CHROMADB_HOST=chromadb
      
    depends_on:
      - lm-studio
      - chromadb
    networks:
      - bot-network
    # Performance optimization: Resource limits
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  lm-studio:
    image: your-lm-studio-image
    ports:
      - "1234:1234"
    networks:
      - bot-network
    # Optimize for emotion analysis workload
    environment:
      - MODEL_CONTEXT_LENGTH=4096      # Optimized context length
      - MODEL_TEMPERATURE=0.2          # Lower temperature for emotion analysis
      - MODEL_MAX_TOKENS=200           # Limit tokens for faster response

  chromadb:
    image: chromadb/chroma:latest
    ports:
      - "8000:8000"
    networks:
      - bot-network
    # Performance optimization
    environment:
      - CHROMA_DB_IMPL=duckdb+parquet
      - CHROMA_PERSIST_DIRECTORY=/chroma-data
    volumes:
      - chroma-data:/chroma-data

networks:
  bot-network:
    driver: bridge

volumes:
  chroma-data:
    driver: local
```

### Dockerfile
```dockerfile
FROM python:3.11-slim

# No GPU dependencies needed!
WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "main.py"]
```

### requirements.txt additions
```txt
# Add these to your existing requirements.txt
aiohttp>=3.8.0
asyncio
```

## üìà Performance Optimization

### Smart Caching
```python
class OptimizedEmotionBot:
    def __init__(self):
        self.emotion_ai = ExternalAPIEmotionAI(...)
        
        # Cache frequently used emotions
        self.emotion_cache_size = 500
        
        # Rate limiting per user
        self.user_rate_limits = {}
    
    async def handle_message_optimized(self, message):
        user_id = str(message.author.id)
        
        # Rate limiting
        if self.is_rate_limited(user_id):
            tier = 'light'  # Use fastest tier when rate limited
        else:
            tier = self.choose_emotion_tier(message.content, user_id)
        
        # Rest of your message handling...
```

### Tier Selection Strategies

#### Strategy 1: User-Based
```python
def choose_tier_by_user(self, user_id):
    """Choose tier based on user subscription/importance"""
    
    # VIP users get heavy tier
    if user_id in self.vip_users:
        return 'heavy'
    
    # New users get medium tier
    if self.is_new_user(user_id):
        return 'medium'
    
    # Regular users get light tier
    return 'light'
```

#### Strategy 2: Time-Based
```python
def choose_tier_by_time(self):
    """Choose tier based on server load"""
    
    hour = datetime.now().hour
    
    # Use lighter tiers during peak hours
    if 18 <= hour <= 22:  # Peak evening hours
        return 'light'
    elif 9 <= hour <= 17:  # Business hours
        return 'medium'
    else:  # Off-peak
        return 'heavy'
```

#### Strategy 3: Context-Based
```python
def choose_tier_by_context(self, text, conversation_history):
    """Choose tier based on conversation context"""
    
    # Use heavy tier for emotional crisis
    crisis_keywords = ['suicide', 'depression', 'hurt myself', 'give up']
    if any(keyword in text.lower() for keyword in crisis_keywords):
        return 'heavy'
    
    # Use heavy tier for important milestones
    milestone_keywords = ['graduation', 'wedding', 'funeral', 'birthday']
    if any(keyword in text.lower() for keyword in milestone_keywords):
        return 'heavy'
    
    # Use medium tier for technical questions
    if '?' in text and len(text.split()) > 10:
        return 'medium'
    
    return 'light'
```

## üîß Security vs Performance Optimization

‚ö†Ô∏è **IMPORTANT**: This integration includes security performance optimizations to ensure security features don't interfere with data quality or service performance.

### Key Optimizations Applied

1. **Smart Caching with Security**
   - TTL-based cache (1 hour) with secure key hashing
   - Larger cache size (1000 entries vs 200)
   - LRU eviction based on hit count and age

2. **Adaptive Timeouts**
   - Tier-specific timeouts: Light (10s), Medium (15s), Heavy (25s)
   - Connection timeout reduced to 5s for faster failure detection
   - Adaptive timeout adjustment based on API performance

3. **Circuit Breaker Pattern**
   - Prevents cascade failures from unreliable APIs
   - 5-failure threshold with 5-minute reset window
   - Automatic fallback to keyword analysis

4. **Optimized Rate Limiting**
   - Reduced max backoff from 8s to 4s
   - Less aggressive backoff multiplier (1.3 vs 1.5)
   - Faster recovery with 0.9 threshold

### Environment Variables

Create a `.env` file (Security-Performance Optimized):
```bash
# Discord
DISCORD_TOKEN=your_discord_token

# Emotion AI (Optimized Configuration)
EMOTION_TIER=medium
EMOTION_OPTIMIZATIONS=true
EMOTION_CACHE_SIZE=1000
EMOTION_CACHE_TTL=3600
LM_STUDIO_URL=http://localhost:1234
HUGGINGFACE_API_KEY=your_hf_token
OPENAI_API_KEY=your_openai_key  # Optional

# Performance Tuning
EMOTION_TIMEOUT_LIGHT=10
EMOTION_TIMEOUT_MEDIUM=15
EMOTION_TIMEOUT_HEAVY=25
CIRCUIT_BREAKER_THRESHOLD=5
CIRCUIT_BREAKER_TIMEOUT=300

# Rate Limiting Optimization
EMOTION_MAX_BACKOFF=4.0
EMOTION_BACKOFF_MULTIPLIER=1.3
EMOTION_RECOVERY_THRESHOLD=0.9

# Your existing config
USE_EXTERNAL_EMBEDDINGS=true
CHROMADB_HOST=localhost
```

## üìä Monitoring & Analytics (Security-Performance Optimized)

### Enhanced Emotion Tracking with Performance Metrics
```python
class OptimizedEmotionAnalytics:
    def __init__(self):
        self.emotion_stats = {
            'tier_usage': {'light': 0, 'medium': 0, 'heavy': 0},
            'emotions_detected': {},
            'api_performance': {'successes': 0, 'failures': 0},
            'response_times': [],
            'cache_performance': {'hits': 0, 'misses': 0},
            'circuit_breaker_events': [],
            'security_events': [],
            'optimization_metrics': {
                'avg_analysis_time': 0,
                'timeout_fallbacks': 0,
                'api_fallbacks': 0
            }
        }
        self.performance_thresholds = {
            'max_analysis_time': 1000,  # ms
            'min_cache_hit_rate': 0.7,  # 70%
            'max_error_rate': 0.05      # 5%
        }
    
    def log_emotion_analysis(self, emotion_analysis, performance_data=None):
        """Enhanced logging with performance metrics"""
        
        tier = emotion_analysis.get('tier_used', 'unknown')
        emotion = emotion_analysis.get('primary_emotion', 'unknown')
        response_time = emotion_analysis.get('analysis_time_ms', 0)
        cache_hit = emotion_analysis.get('cache_hit', False)
        
        # Update basic stats
        self.emotion_stats['tier_usage'][tier] += 1
        self.emotion_stats['emotions_detected'][emotion] = \
            self.emotion_stats['emotions_detected'].get(emotion, 0) + 1
        self.emotion_stats['response_times'].append(response_time)
        
        # Update performance stats
        if cache_hit:
            self.emotion_stats['cache_performance']['hits'] += 1
        else:
            self.emotion_stats['cache_performance']['misses'] += 1
        
        # Check for performance issues
        if response_time > self.performance_thresholds['max_analysis_time']:
            self.emotion_stats['optimization_metrics']['timeout_fallbacks'] += 1
            print(f"‚ö†Ô∏è Slow analysis: {response_time:.1f}ms (tier: {tier})")
        
        # Log circuit breaker activity
        circuit_breakers = emotion_analysis.get('circuit_breakers_active', [])
        if circuit_breakers:
            self.emotion_stats['circuit_breaker_events'].append({
                'timestamp': datetime.now().isoformat(),
                'active_breakers': circuit_breakers,
                'tier': tier
            })
        
        # Security event logging
        if emotion_analysis.get('analysis_method', '').endswith('_fallback'):
            self.emotion_stats['security_events'].append({
                'timestamp': datetime.now().isoformat(),
                'event': 'api_fallback',
                'method': emotion_analysis.get('analysis_method'),
                'tier': tier
            })
    
    def get_performance_health_check(self):
        """Get real-time performance health status"""
        
        total_requests = sum(self.emotion_stats['tier_usage'].values())
        if total_requests == 0:
            return {'status': 'no_data', 'health_score': 0}
        
        # Calculate metrics
        cache_total = self.emotion_stats['cache_performance']['hits'] + \
                     self.emotion_stats['cache_performance']['misses']
        cache_hit_rate = (self.emotion_stats['cache_performance']['hits'] / 
                         max(1, cache_total))
        
        avg_response_time = (sum(self.emotion_stats['response_times']) / 
                           len(self.emotion_stats['response_times']))
        
        error_rate = len(self.emotion_stats['security_events']) / total_requests
        
        # Calculate health score (0-100)
        health_score = 100
        
        if cache_hit_rate < self.performance_thresholds['min_cache_hit_rate']:
            health_score -= 30
        
        if avg_response_time > self.performance_thresholds['max_analysis_time']:
            health_score -= 40
        
        if error_rate > self.performance_thresholds['max_error_rate']:
            health_score -= 30
        
        return {
            'status': 'healthy' if health_score > 70 else 'warning' if health_score > 40 else 'critical',
            'health_score': max(0, health_score),
            'metrics': {
                'cache_hit_rate': cache_hit_rate,
                'avg_response_time_ms': avg_response_time,
                'error_rate': error_rate,
                'total_requests': total_requests
            },
            'recommendations': self._generate_performance_recommendations(
                cache_hit_rate, avg_response_time, error_rate
            )
        }
    
    def _generate_performance_recommendations(self, cache_hit_rate, avg_response_time, error_rate):
        """Generate performance optimization recommendations"""
        
        recommendations = []
        
        if cache_hit_rate < 0.5:
            recommendations.append("üî• CRITICAL: Cache hit rate very low - consider increasing cache size or TTL")
        elif cache_hit_rate < 0.7:
            recommendations.append("‚ö†Ô∏è WARNING: Cache hit rate below target - optimize caching strategy")
        
        if avg_response_time > 2000:
            recommendations.append("üî• CRITICAL: Very slow responses - check API connectivity and timeouts")
        elif avg_response_time > 1000:
            recommendations.append("‚ö†Ô∏è WARNING: Slow responses - consider tier optimization or circuit breaker tuning")
        
        if error_rate > 0.1:
            recommendations.append("üî• CRITICAL: High error rate - investigate API failures and fallback mechanisms")
        elif error_rate > 0.05:
            recommendations.append("‚ö†Ô∏è WARNING: Elevated error rate - monitor API health")
        
        if not recommendations:
            recommendations.append("‚úÖ Performance is optimal - continue monitoring")
        
        return recommendations
    
    def get_daily_report(self):
        """Enhanced daily emotion analytics report"""
        
        total_requests = sum(self.emotion_stats['tier_usage'].values())
        if total_requests == 0:
            return {'message': 'No emotion analysis data for today'}
        
        avg_response_time = sum(self.emotion_stats['response_times']) / \
                           len(self.emotion_stats['response_times'])
        
        cache_total = self.emotion_stats['cache_performance']['hits'] + \
                     self.emotion_stats['cache_performance']['misses']
        cache_hit_rate = self.emotion_stats['cache_performance']['hits'] / max(1, cache_total)
        
        return {
            'summary': {
                'total_analyses': total_requests,
                'avg_response_time_ms': round(avg_response_time, 2),
                'cache_hit_rate': round(cache_hit_rate, 3),
                'circuit_breaker_events': len(self.emotion_stats['circuit_breaker_events']),
                'security_events': len(self.emotion_stats['security_events'])
            },
            'tier_distribution': self.emotion_stats['tier_usage'],
            'top_emotions': sorted(
                self.emotion_stats['emotions_detected'].items(),
                key=lambda x: x[1], reverse=True
            )[:5],
            'performance_health': self.get_performance_health_check(),
            'optimization_suggestions': self._get_optimization_suggestions()
        }
    
    def _get_optimization_suggestions(self):
        """Generate optimization suggestions based on usage patterns"""
        
        suggestions = []
        tier_usage = self.emotion_stats['tier_usage']
        total = sum(tier_usage.values())
        
        if total == 0:
            return suggestions
        
        # Analyze tier usage patterns
        light_pct = tier_usage['light'] / total
        medium_pct = tier_usage['medium'] / total
        heavy_pct = tier_usage['heavy'] / total
        
        if heavy_pct > 0.5:
            suggestions.append("Consider optimizing tier selection - high heavy tier usage may impact performance")
        
        if light_pct < 0.2:
            suggestions.append("Consider using light tier more for simple messages to improve performance")
        
        # Analyze response times by tier
        if self.emotion_stats['response_times']:
            avg_time = sum(self.emotion_stats['response_times']) / len(self.emotion_stats['response_times'])
            if avg_time > 800:
                suggestions.append("Consider reducing timeouts or optimizing API calls for better performance")
        
        return suggestions
```

## üéØ Benefits for Your Use Case

### ‚úÖ Solves Your Constraints
- **Apple Silicon Docker**: No local transformers/torch
- **External APIs**: Uses your LM Studio + cloud APIs
- **Resource Tuning**: 3 tiers for flexible resource usage
- **Phase 3 Integration**: Works with your existing memory system

### ‚úÖ Production Ready
- **Graceful Fallbacks**: APIs fail ‚Üí falls back to keywords
- **Rate Limiting**: Prevents API overuse
- **Caching**: Reduces redundant API calls
- **Monitoring**: Track usage and performance

### ‚úÖ Emotional Intelligence
- **95%+ Accuracy**: With medium/heavy tiers
- **Multi-emotion Detection**: Beyond simple keyword matching
- **Context Awareness**: Uses conversation history
- **Adaptive Responses**: Temperature adjustment based on emotion

## üöÄ Getting Started (Security-Performance Optimized)

1. **Copy Files**: 
   - `optimized_external_api_emotion_ai.py` to your project (replaces `external_api_emotion_ai.py`)
   - `security_performance_optimizer.py` for ongoing optimization analysis

2. **Install Dependencies**: 
   ```bash
   pip install aiohttp asyncio hashlib
   ```

3. **Set Environment Variables**: 
   - Use the optimized `.env` configuration above
   - Enable `EMOTION_OPTIMIZATIONS=true` for performance features

4. **Integrate**: 
   - Replace `ExternalAPIEmotionAI` with `OptimizedExternalAPIEmotionAI`
   - Add performance monitoring with `OptimizedEmotionAnalytics`

5. **Test Performance**: 
   ```python
   # Test the optimized implementation
   emotion_ai = OptimizedExternalAPIEmotionAI(enable_optimizations=True)
   await emotion_ai.initialize()
   
   # Monitor performance
   metrics = emotion_ai.get_performance_metrics()
   print(f"Cache hit rate: {metrics['cache_hit_rate']:.1%}")
   print(f"Avg response time: {metrics['avg_analysis_time_ms']:.1f}ms")
   ```

6. **Optimize Based on Usage**: 
   - Run `security_performance_optimizer.py` weekly
   - Adjust tier selection based on analytics
   - Fine-tune timeouts and cache settings

### Performance Validation Checklist

‚úÖ **Security Features Working**:
- [ ] API key validation active (check logs for validation events)
- [ ] Circuit breakers functioning (test with invalid API endpoints)  
- [ ] Graceful fallbacks to keyword analysis when APIs fail
- [ ] Rate limiting preventing API overuse

‚úÖ **Performance Optimizations Active**:
- [ ] Cache hit rate > 70% after first hour of operation
- [ ] Average analysis time < 500ms for light tier, < 800ms for medium tier
- [ ] Circuit breakers reducing failed API calls
- [ ] Timeout fallbacks working within tier-specific limits

‚úÖ **Data Quality Maintained**:
- [ ] Emotion detection accuracy remains high (spot check with test cases)
- [ ] No significant increase in "neutral" classifications (indicates degraded analysis)
- [ ] Conversation context still being analyzed when available
- [ ] Phase 3 integration working if available

### Troubleshooting Performance Issues

**High Response Times (>1000ms average)**:
1. Check circuit breaker status - may indicate API problems
2. Reduce tier usage (use more light tier during peak hours)
3. Increase timeout values if APIs are consistently slow
4. Check network connectivity to external APIs

**Low Cache Hit Rate (<50%)**:
1. Increase cache size: `EMOTION_CACHE_SIZE=2000`
2. Increase TTL: `EMOTION_CACHE_TTL=7200` (2 hours)
3. Check if users are sending very unique messages (expected behavior)

**High Error Rate (>5%)**:
1. Check API key validity and quotas
2. Verify external API endpoints are accessible
3. Review circuit breaker thresholds - may be too aggressive
4. Check for network connectivity issues

**Memory Usage High**:
1. Reduce cache size: `EMOTION_CACHE_SIZE=500`
2. Reduce cache TTL: `EMOTION_CACHE_TTL=1800` (30 minutes)
3. Enable cache cleanup more frequently
4. Monitor for cache memory leaks

**This implementation provides maximum emotional intelligence while maintaining excellent performance and security!** üß†üíô‚ö°